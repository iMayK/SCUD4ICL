import os
import argparse
import time
import json
import logging
from typing import List, Dict, Tuple, Any
import concurrent.futures
from tqdm import tqdm

from utils import load_jsonl_file, load_json_file, dump_json_file
from qd_prompts import FEW_SHOT_EXAMPLES, SYS_PROMPT, USER_TEMPLATE
from llm_utils_new import LLMResponseManager, get_prompt, pretty_print_prompt


MAX_TRIES = 10
MAX_SLEEP = 50


def merge_log_files(num_workers, log_file_name, main_logger):
    """utility function to merge log files generated by workers"""
    main_logger.info("Starting to merge log files")
    with open(f"{log_file_name}_main.log", 'a') as outfile:
        for i in range(num_workers):
            worker_log = f"{log_file_name}_worker_{i+1}.log"
            if os.path.exists(worker_log):
                with open(worker_log, 'r') as infile:
                    outfile.write(infile.read())
                os.remove(worker_log)
    main_logger.info("Finished merging log files")


def setup_logger(base_name, worker_id=None):
    """utility function to setup logger"""
    logger_name = f"worker_{worker_id}" if worker_id else "main"
    logger = logging.getLogger(logger_name)

    if not logger.handlers:
        logger.setLevel(logging.INFO)
        logger.propagate = False
        file_handler = logging.FileHandler(f"{base_name}_{logger_name}.log")
        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
        file_handler.setFormatter(formatter)
        logger.addHandler(file_handler)
    return logger


def get_file_names(
    dataset_name: str,
    split_info: Tuple[str, str, str],
    model_name: str,
    output_file_suffix: str
) -> Tuple[str, str, str, str]:

    split_name, split_split_name, split_mode = split_info

    base_path = f"data/{dataset_name}"
    common_suffix = f"{model_name}_{split_name}_{split_split_name}_{split_mode}"

    dataset_path = f"{base_path}/all.jsonl"
    split_mode_path = f"{base_path}/subset_indices.json"
    log_file_name = f"{base_path}/LOGS/QD_{common_suffix}"
    output_file_name = f"{base_path}/decomposed/{common_suffix}_{output_file_suffix}.json"

    return dataset_path, split_mode_path, log_file_name, output_file_name


def wrapup_processing(
    data_processed_so_far: List[Dict[str, Any]],
    new_data_processed: List[Dict[str, Any]],
    output_file_name: str,
    fail_count: int,
    logger: logging.Logger
):
    logger.info("\n%s samples newly processed", len(new_data_processed))

    total_data_processed = data_processed_so_far + new_data_processed
    dump_json_file(output_file_name, total_data_processed)

    logger.info("\n%s total data processed so far", len(total_data_processed))
    logger.warning("Count fail: %s", fail_count)


def llm_response_w_retry(
    prompt: List[Dict[str, Any]],
    llm_manager: LLMResponseManager,
    logger: logging.Logger
) -> Tuple[Dict[str, Any], int]:
    """
    Attempt to get a response from LLM with exponential sleep on failure
    up to MAX_TRIES times with a maximum sleep time of MAX_SLEEP seconds.
    """
    current_sleep = 5
    decomposition_dict = {}
    is_fail = 1
    for attempt in range(1, MAX_TRIES+1):
        logger.info("Attempt %s", attempt)
        try:
            llm_decomposition = llm_manager.get_response(prompt, stop=["```"])
            try:
                decomposition_dict = json.loads(llm_decomposition)
                is_fail = 0
                break
            except json.JSONDecodeError:
                try:
                    # Second attempt by prefixing with '{' because of the prompt structure
                    decomposition_dict = json.loads("{" + llm_decomposition)
                    is_fail = 0
                    break
                except json.JSONDecodeError:
                    logger.warning("json decode error, trying again...")
                    logger.warning("llm output:\n%s", llm_decomposition)
                    continue
        except Exception as e:
            logger.warning("An error occurred: %s", e)
            time.sleep(current_sleep)
            current_sleep = min(current_sleep * 2, MAX_SLEEP)
    else:
        logger.error("Failed to get a valid response after %s attempts.", MAX_TRIES)
        logger.error("llm output:\n%s", llm_decomposition)

    decomposition_dict_str = json.dumps(decomposition_dict, indent=4)
    logger.info("\nFinal LLM responese: %s\n\n", decomposition_dict_str)
    logger.info("=="*70)

    return decomposition_dict, is_fail


def process_item(
    item: Dict[str, Any],
    llm_manager: LLMResponseManager,
    logger: logging.Logger
) -> int:
    qid = item['qid']
    query = item['query']
    code = item.pop('simplified')
    item['code'] = code

    user = USER_TEMPLATE.format(query, code)
    prompt = get_prompt(user, FEW_SHOT_EXAMPLES, SYS_PROMPT)

    logger.info("\n\nqid: %s\n", qid)
    pretty_print_prompt(prompt, logger)

    decomposition_dict, is_fail = llm_response_w_retry(prompt, llm_manager, logger)

    item['decomposition'] = decomposition_dict
    return is_fail


def process_dataset_sequential(
    remaining_ids: List[str],
    qid_to_example: Dict[str, Dict[str, Any]],
    llm_manager: LLMResponseManager,
    main_logger: logging.Logger,
    data_processed_so_far: List[Dict[str, Any]],
    output_file_name: str,
):
    """process dataset sequentially"""
    fail_count = 0
    new_data_processed = []
    try:
        for qid in tqdm(remaining_ids, total=len(remaining_ids)):
            item = qid_to_example[qid]
            is_fail = process_item(item, llm_manager, main_logger)
            new_data_processed.append(item)
            fail_count += is_fail
    except KeyboardInterrupt:
        main_logger.error("KeyboardInterrupt: wrapping up processing...")
        return new_data_processed, fail_count

    # return new_data_processed, fail_count
    wrapup_processing(
        data_processed_so_far, new_data_processed, output_file_name, fail_count, main_logger
    )


def process_dataset_parallel(
    remaining_ids: List[str],
    qid_to_example: Dict[str, Dict[str, Any]],
    llm_manager: LLMResponseManager,
    main_logger: logging.Logger,
    num_workers: int,
    data_processed_so_far: List[Dict[str, Any]],
    output_file_name: str,
    log_file_name: str
):
    """process dataset using multiple threads"""

    loggers = [setup_logger(log_file_name, i+1) for i in range(num_workers)]

    new_data_processed = []
    fail_count = 0
    try:
        with concurrent.futures.ThreadPoolExecutor(max_workers=num_workers) as executor:
            futures = {
                executor.submit(
                    process_item, qid_to_example[qid], llm_manager, loggers[i % num_workers]
                ): qid
                for i, qid in enumerate(remaining_ids)
            }

            for future in tqdm(
                concurrent.futures.as_completed(futures), total=len(remaining_ids), ncols=100
            ):
                qid = futures[future]
                new_data_processed.append(qid_to_example[qid])
                is_fail = future.result()
                fail_count += is_fail
    except KeyboardInterrupt:
        main_logger.error("KeyboardInterrupt: wrapping up processing...")
        # return new_data_processed, fail_count
        merge_log_files(num_workers, log_file_name, main_logger)
        wrapup_processing(
            data_processed_so_far, new_data_processed, output_file_name, fail_count, main_logger
        )
        exit()

    # return new_data_processed, fail_count
    merge_log_files(num_workers, log_file_name, main_logger)
    wrapup_processing(
        data_processed_so_far, new_data_processed, output_file_name, fail_count, main_logger
    )


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Run question decomposition on specified split")
    parser.add_argument('--split_name', type=str, required=True, help='split name')
    parser.add_argument(
        '--split_split_name', type=str, required=True, help='specific split of compositional splits'
    )
    parser.add_argument('--split_mode', type=str, required=True, help='split mode')
    parser.add_argument('--model_name', type=str, default="gpt4_0125", help='decomposer?')
    parser.add_argument('--api_type', type=str, default="azure", help='api type?')
    parser.add_argument('--api_endpoint', type=str, default="", help='')
    parser.add_argument('--num_workers', type=int, default=1, help='number of workers to use')
    args = parser.parse_args()

    split_name = args.split_name
    split_split_name = args.split_split_name
    split_mode = args.split_mode
    model_name = args.model_name
    api_type = args.api_type
    api_endpoint = args.api_endpoint
    num_workers = args.num_workers

    DATASET_NAME = "smcalflow"
    FILE_SUFFIX = "trying"

    llm_manager = LLMResponseManager(
        model_name=model_name, api_type=api_type, api_endpoint=api_endpoint
    )

    dataset_path, split_mode_path, log_file_name, output_file_name = get_file_names(
        DATASET_NAME,
        (split_name, split_split_name, split_mode),
        model_name,
        FILE_SUFFIX
    )

    main_logger = setup_logger(log_file_name)

    SPLIT_MODE_TO_KEY = {
        "top-5-subset": '0C_top5',
        "top-10-subset": '0C_top10',
        "all-python-programs": "0C_py"
    }
    all_original_examples = load_jsonl_file(dataset_path)
    split_mode_ids = load_json_file(split_mode_path)[SPLIT_MODE_TO_KEY[split_mode]]

    keys_to_keep = ['qid', 'query', 'simplified']
    all_original_examples = [
        {key: item[key] for key in keys_to_keep if key in item}
        for item in all_original_examples
    ]
    qid_to_example = {ex["qid"]: ex for ex in all_original_examples}

    data_id_processed_so_far = []
    data_processed_so_far = []
    if os.path.exists(output_file_name):
        data_processed_so_far = load_json_file(output_file_name)

        data_id_processed_so_far = [item['qid'] for item in data_processed_so_far]

        main_logger.info("\n%s samples already processed", len(data_processed_so_far))

    skip_ids = []
    skip_data = load_json_file("data/smcalflow/decomposed/mixtral:8x22b_source_domain_with_target_num0_train_top-5-subset.json")
    skip_ids = [item['qid'] for item in skip_data if item['decomposition'] != {}]

    remaining_ids = list(set(split_mode_ids) - set(data_id_processed_so_far) - set(skip_ids))

    main_logger.info("\ntotal split mode ids: %s", len(split_mode_ids))
    main_logger.info("processing remaining %s ids: %s", split_mode, len(remaining_ids))

    if num_workers == 1:
        process_dataset_sequential(
            remaining_ids, qid_to_example, llm_manager, main_logger,
            data_processed_so_far, output_file_name
        )
    else:
        process_dataset_parallel(
            remaining_ids, qid_to_example, llm_manager, main_logger, num_workers,
            data_processed_so_far, output_file_name, log_file_name
        )
